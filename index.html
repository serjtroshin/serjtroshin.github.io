---
layout: default
title: Blog Posts
---

<div id="home">
  <h2><i class="fa fa-bookmark"></i> About me</h2>
  <p>I am PhD student at the <a href="https://ltl.science.uva.nl/">Language Technology Lab</a> at the University of Amsterdam supervised by <a href="https://vene.ro/">Dr. Vlad Niculae</a>. My research is mainly focused on probabilistic models and constrained generation for text. More broadly, I am interested in natural language processing and deep learning for source code.</p>
  
  <h2><i class="fa fa-bookmark"></i> News</h2>
  <ul id="blog-posts" class="posts">
    <li><span>Sep 2025 &raquo;</span> I presented our work  <a href="https://openreview.net/forum?id=IyOC5GCzv4#discussion">Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs</a> at COLM 2025 in Montreal! We notice that there are “risky” generation timestamps, when sampling is likely to deteriorate the quality. As a solution, we develop an inference time sampling method that dynamically switches between greedy and high temperature sampling depending on the timestamp. </li>
    <li><span>Aug 2025 &raquo;</span> Our paper,  <a href="https://openreview.net/forum?id=cjRsEGLT8B">On the Low-Rank Parametrization of Reward Models for Controlled Language Generation</a>, was accepted to the TMLR journal! By looking closely at the parameterization of the reward models (q-style vs v-style), we show that q-style reward models have limited rank-capacity, namely, they cannot model all reward functions in context. </li>
    <li><span>Jul 2024 &raquo;</span> I attended LOGML summer (deep learning and geometry) school, where I worked on a project for bayesian inference of parameters of differential equations.</li>
    <li><span>5 Dec 2023 &raquo;</span> My paper is accepted <a href="https://openreview.net/forum?id=KrequDpWzt"> to the TMLR journal </a>! We propose a new non-isotropic distribution for probabilistic modeling on Riemannian manifolds. </li>
    <li><span>Jul 2023 &raquo;</span> I spent a great time at the LxMLS 2023 summer school in Lisbon! </li>
    <li><span>Jun 2023 &raquo;</span> I attended OxML summer school (MLx Finance & NLP), which took place at Oxford. </li>
    <li><span>8 Dec 2022 &raquo;</span> We presented our <a href="https://arxiv.org/abs/2202.08975"> workshop paper on BlackboxNLP 2022</a> where we probed deep learning models for source code.</li>
    <li><span>1 Dec 2022 &raquo;</span> I started my PhD at the <a href="https://ltl.science.uva.nl/">Language Technology Lab</a> at the University of Amsterdam supervised by <a href="https://vene.ro/">Dr. Vlad Niculae</a>! </li>
    <li><span>29 Apr 2022 &raquo;</span> We presented our <a href="https://openreview.net/forum?id=rd-G1nO-Jbq_">spotlight paper on CodeBPE</a> on subtokenization for source code models at the 
      <a href="https://nips.cc/Conferences/2021">DL4Code workshop of ICLR 2022</a>
     (online)!</li>
    <li><span>22 Aug 2021  &raquo;</span> We with Nadezhda Chirkova presented our paper
      <a href="https://arxiv.org/abs/2010.07987">Empirical Study of Transformers for Source Code</a>
      at <a href="https://2021.naacl.org/">ESEC/FSE 2021</a> (online).
      Here is our <a href="https://github.com/bayesgroup/code_transformers">code</a></li>
    <li><span>7 Jun 2021  &raquo;</span> <a href="https://arxiv.org/abs/2010.12663">A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code</a> paper presented at <a href="https://2021.naacl.org/">NAACL 2021</a> with Nadezhda Chirkova</li>
</ul>




  </ul>
</div>
